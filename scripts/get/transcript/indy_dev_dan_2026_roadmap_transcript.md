# Indy Dev Dan: 2026 Roadmap for Top 2% Agentic Engineers

---

## Introduction

### Self-Positioning & Goal

What's up engineers? Indy Dev Dan here. I would place myself in the top 10% of engineers using agentic technology. Where would you place yourself? Do you think you're top 15, top 10, top five? I want to become a top 2% engineer and I want to take you along with me. In fact, I want you to surpass what I can do. The best way to uplevel what you can do is to create a plan. And that's exactly what I've done. Let me share with you my roadmap for becoming a top 2% agentic engineer in 2026. This is everything I'm thinking about. This is everything that I believe based on all the conversations, all the work I've done over the past years. This is what I think it looks like to become a top 2% a Gentic engineer in 2026.

---

## Part 1: The Landscape

### Gemini 3 Flash Announcement

Let's first start with a couple big announcements that are going to tie in really well. So, first off, Gemini 3 Flash is absolutely insane. This model should not exist. Why do I say that? Why should this model not exist? It's because it is a top three intelligence model. Top five price. This hasn't happened before, by the way. You don't get intelligence and price. On top of that, what else do we have? We have the top speed. Some measurements are coming in at 200 tokens per second. It is a lightning fast model. Gemini 3 flash inputs text, images, audio, video, and PDF.

#### Open Router Usage Data

And just real quick here, you can see this in the usage numbers, right? You have to ignore all the benchmarks, all these auxiliary numbers at some point and just look at the raw usage data. If we search for Gemini 3 flash preview, check this out on Open Router, 385% token increase usage. It's up in 13th place. I'm predicting this goes up to the top 10, potentially even the top five by the end of the year. So, it's a fantastic model. I've been using this thing alongside Opus 4.5.

---

### What These Releases Mean for Engineers

So, you know, what do these releases mean for engineers? Let's set up for 2026, right? What do these releases mean? What's coming next for us? All right, we have Gemini 3 Flash, Opus 4.5, and GBG 5.2. These are all stateofthe-art correct models. There has never been more opportunity for us engineers. We're going to break down that opportunity for 2026 right now in this video.

#### No Winners = Great for Engineers

These releases are telling us there are no winners. And this is a great thing, right? Competition drives innovation and it also drives price down. That means intelligence per token is going up. And that means you and I, the engineer, we can do more.

#### Price Analysis

Take a look at these prices. Gemini 2.5 flash 30 cents. Gemini 3 flash 50 cents. A lot of engineers see a price increase here. I see a price decrease. Why is that? It's because Gemini 3 Flash is competing with top state-of-the-art models. Okay, intelligence per token has gone up. That means cost per intelligence or cost per token has gone down. You get more for less. That's a big theme. Keep that in mind as we work through this.

#### The Key Insight: Models Are No Longer the Limitation

All right, the model ecosystem is competitive. So, this is great. Like I mentioned, this gives us optionality. Out of all these things, there is one that is the most important. What do these releases mean for us engineers? Models are no longer the limitation. Tricky question for you. Do you know what the limitation is?

---

### The Central Thesis: Year of Trust

And this brings us to the central thesis for my plan for 2026 for top 2% engineering. If you want to be the very best or at least approach the best or improve, this is what I think you need. Andrew Carpathy calls the next 10 years the year of agents. I think that's right. But we need to dial in what is 2026? What is the theme for 2026? I believe this is the year of trust.

#### What Trust Means

What do I mean by that exactly? Every one of these bets, every prediction, every action I think top engineers will be taking comes down to one question. Do you trust your agents? This is the year of trust. Do you trust your agents?

What do I mean by this? There are powerful agentic systems, agentic workflows, AI developer workflows that you can build that increases the amount of work you can hand off to your agents. The question is always this. Why haven't you pushed your agents to do more? Why haven't you built that next system? What's stopping you from automating that task you did yesterday? Right? And I think all of it, and I mean all of it, boils down to one word. You and I, we do not trust our agents. And more specifically, this is a gray question. It's not a yes or no question. The true question here is how much do you trust your agents? So, this is why we are framing everything as the year of trust.

---

#### Why Trust Matters: Trust = Speed = Iteration = Impact

Let's pause for a second. Why do we care about trust? We care about trust because trust equals speed. Speed equals iteration and iteration equals impact.

Think about a team you've worked with. Think about a technology you've used, right? I love the example claw code running Opus 4.5. I trust that model and that agent harness. Why? Because I know it's going to deliver. This gives me concrete speed. That speed gives me iteration on the problems I'm solving. And what does that give me? The more iterations, the more cycles that you get, that directly increases your impact.

All right. So why do we care about trust? It's because it gives us impact. So, this is why I think this central thesis is very powerful for thinking about how to become a top 2% engineer in 2026.

---

### Overview of the 10 Bets

This is everything we're going to discuss here. I'm just going to lay it all out for you. If you're interested in these ideas, stick around. If you believe and you're on board with my trust thesis, like this is what it's all about. All right? Top 2% engineers are going to make these bets to effectively increase their trust. If you don't like these ideas, if you're not interested, do me a quick favor. Copy that URL. uh for this video and send it to your competition real quick.

#### Track Record (2023-2025)

Okay, so before we jump right in, let me just say, you know, I do this every single year. All right, in 2023, I made 24 bets, 16 were right. 2024, I made 41 bets. Tons of bets there. 36 were right. All right, this year, as you'll see if you stick around, I made 15 bets, 13 were right. Accuracy is going up. And this year, I'm going to make 10 concrete bets as you saw in the previous slide. And we're going to see where we end up at the end of the year.

#### Who I Am

All right. You know, a lot of new uh engineers that watch this channel, you guys think I'm a YouTuber. Uh I'm not. I'm an engineer and I use this technology every single day. I bet on it. every single video you see every you know pattern idea model experiment like I I'm actively doing using and looking for the best tools for the job of engineering right now that tool is very clearly agents the best way to get the largest advantage is to show up before anyone else has even started those who plan the future tend to create it sometimes showing up on time is showing up too late engineering building businesses Because being inside of organizations is all about information advantages. All right? And you want the largest advantage for the longest period of time. The best way to do this is to prepare for opportunity before it presents itself so that you're ready when it arrives. Anyway, that's my track record. I'll leave my previous year's prediction videos in the description if you're interested. We're going to cover 25 at the end of this video.

---

## Part 2: The 10 Big Bets

### Big Bet #1: Anthropic Becomes a Monster

Let's jump in to big bet number one. How are we going to increase trust in our agentic systems to become a top 2% engineer? What would a top 2% engineer do in 2026 that others will not be? Let's start with a spicy statement here. Anthropic becomes a monster. I'll tie this back into trust and agentic systems in a moment here. But this is the first big idea I want to talk with you about and the first big bet I think top engineers will make.

#### Why Bet on Anthropic

All right, cloud code was luck. Sonic 3.5 was luck, but everything since then has been a masterclass in execution. A lot of engineers don't look beyond the models, but this company runs on Nvidia, Google, and AWS. Okay, they are everywhere. Cloud code unlocked agentic coding for the masses. This is the top agentic coding tool and there's a reason for that. Anthropic isn't trying to win everyone. They have been focused on one avatar. That's you and me. That's the engineer. That's why top 2% engineers will continue to bet on Enthropic. I am betting on this.

#### How This Ties to Trust

How does this tie into trust? It's quite simple, right? When you bet on one company with the best track record for engineers, right? The best tool calling, the best agent harness, and consistent execution, your agents will have the best foundation. And that means you can just spend less time fighting tooling and more time shipping.

#### The Anthropic Timeline (2024-2025)

And I just really want, you know, I put together this timeline of the insane work that Anthropic has been doing. Let's just take a look at some of the top events.

All right, so 2024 June, we had the Sonic 3.5 release. Huge. One of the best tool calling models in existence. Okay, I think the best at the time. Fast forward February 2025, we got Cloud 3.7 and more importantly, Claude code. This was the first big introduction for a lot of engineers. I'd seen it a little bit before this, but this was the big introduction to a gentic coding, right? Context, model, prompt, plus tools, right? The big three became the core four. And then fast forward a little bit more, we got the cloud code SDK. What does that mean? This is custom agents. We're going to talk about this more in a second. Fast forward a little bit further, we got sub agents. Okay, we got Opus 4.1, Sonnet 45, and now we have Opus4. Of course, we have skills. We have a couple other really important releases in here as well.

#### It's Not Just About the Model Anymore

But notice that this is not just about the model anymore. Okay? Like I mentioned, the game is not just about the model harness anymore. It's about what you and I can do with the technology. It's about trust. It's about scale. All right? And a lot of these releases here really showcase that, right? It's not just about the model anymore. What's the model running in? Right? Is it running in a good harness? Is it caching the tokens for you? Can you spin up more compute from that one harness? Right, sub aents. So, I just want to emphasize this, right? It's not just about putting out the best model anymore. It hasn't been. And you know, Anthropic has been the leader in this space. Let's make that absolutely clear. For a Gentics, for a gentic coding, for a this new realm that I predicted would emerge years ago called aentic engineering, cloud code has been the leader here. Okay, let's make that super clear. So I think top 2% engineers are going to continue to bet on Anthropic as the kind of leader in the space putting out the best tooling for this new role of engineering.

---

### Big Bet #2: Tool Calling is the Opportunity

All right, let's move on. Big bet number two in that same vein, right? Tool calling is the opportunity. Okay, your agents taking actions on your behalf is the opportunity.

#### Open Router Data: Only 15% Are Tool Calls

Now this is something really interesting. Only 15% of output tokens are tool calls according to Open Router. All right. Now, I I want to concretely pull this up. All right, this is right from their state of AI. I've read through this and there's a couple shocking things I want to mention here.

##### Reasoning Models Dominance

Okay, what one in particular, check this graph out, right? This is an obvious great graph. Reasoning models now represent over half of all language model usage. And this makes sense, right? Uh before 01 was released, one of the first reasoning models on the channel, we have proof of this. Uh we were prompt chaining, okay? We were building our own chains of reasoning by calling language models over and over and over making them think about their answer, right? The the early versions of what agents are now, right? Prompts in a loop. Then 01 was released and we were right. And this turned into a full paradigm. Okay? Now they take up over half of all reasoning tokens. This is fantastic, right? This makes sense. You scale your compute to scale your impact.

##### The 15% Opportunity

Now, if we scroll down here, we see something absolutely mind-blowing. rise of adoption of tool calling. Only 15% of all output tokens were tool calls. Okay, that's what this is saying. This is tool invocations over time. Um, do you see what I see? Do you smell what I smell? This is the greatest opportunity for engineers that's going to exist for the next year. Okay, I don't know about you, but most of my model usage is tool calling. It It's raw tool calling. Why is that? It's because tool calls is a rough proxy for impact for actions, right?

#### Tool Calls = Impact

But think about what a tool call is. It's your agents taking real actions as you would, right? Your agents are calling bash commands. Your agents are reviewing your work in the browser. They're taking screenshots. Your agents are now running arbitrary skills that can call any CLI script or tool that you build up. Right? This number is extraordinarily low. Right?

That brings me back to this opportunity. You can roughly think of tool calls as impact, right? This is your agents taking actions. This is what turns the big three into the core four. Context, model, prompt, tools. This is what creates agentic coding. Your agents are not AI coding anymore. They're agentic coding. Right? Kind of spoiling the next bet. Custom agents with custom tools are the best way to maximize tool calls and impact.

#### Tool Calling = Trust Measurement

Okay? So I am fully betting on if you want to be a top 2% engineer, you will push your agents to call longer and longer chains of tools, therefore increasing your total impact. Tool calling is the opportunity.

All right, that's what this is. This is also the direct measure of how much you trust your agents. Okay, the longer the sequence of correct tool calls is for whatever problem you're trying to solve, the more trust you have in your agents. Okay, I think boil down every bet we're making here. Boil down everything top 2% engineers will be doing. It comes down to this. How long can you let your agents go? One to many agents, right? How long can they go before making a mistake that you have to correct? This is what I mean when I say trust.

#### The Right Model for Tool Calling

Okay. So then the question becomes, what's the best way to increase your chance of your agents calling the right long sequence of tools? Yes, of course, the model plays a big role in in that, right? This is why there are specific models that consistently hit the top of the leaderboard.

##### Top Models for Tool Calling

And we can see this, right? Again, shout out to the open router state of AI post. Top 10 mostused models that call the tool. Notice the trend here, right? It started out with GPT40 mini in the beginning of the year and then guess which models took over pink anthropic claw 3.7 and then all of a sudden Clawude 4 sonnet in blue here just annihilated this right it took over the entire market okay and then guess what happened once again what what what is this model claude 4.5 and we know that claude opus is now emerging as the top best model for long sequences of tool calls this chart tells is a very important deep story. Again, as I mentioned in the first prediction, Enthropic is creating the best models, the best tooling for long chains of tool calls. And that ties right into our second big bet. Tool calling is the opportunity. Okay, let's make this super clear. You know, when everyone talks about agents, this is what they're talking about. Reliable long chains of tool calls. You know, the model is important, but what comes next? What is the next most important thing?

---

### Big Bet #3: Custom Agents Above All

Again, I talk about this on the channel all the time. Everything is the core four. What puts together the core four? Of course, your own custom agents. I'm betting big on custom agents above all.

#### The 50-Line Agent That Changes Everything

Would you believe a custom agent with 50 lines of code, three tools, and a 150 line system prompt could completely automate a massive problem you face every day? Would you believe it if I told you that? Now, I hope you do because this is true. I have built these, okay? And I'll be sharing more of them on the channel, of course, over 2026. Be sure to like and comment so the algorithm knows that you're interested. But this is a real thing.

#### The Value Proposition

And this is the value proposition of custom agents. Why are they so important? Custom agents solve custom problems. And custom problems make you a ton of money. Okay? This is what this is what engineering is all about, right? You you you build custom solutions to solve custom problems that someone's willing to pay for. Okay. And custom agents give you that exactly. They unlock that opportunity with compute, with prompts, with agents.

#### Agent SDKs: Table Stakes

Okay. Using agent coding tools is table stakes now for engineers, right? The next step up is learning to build custom agents with some agent SDK. Okay? Every big AI lab has an agent SDK. Pick one, lock it in, use it. I use two in particular. The first one is, of course, the Clawed Asian SDK. The second we'll talk about later in the future on the channel.

#### Why This Is The Most Important Bet

Top 2% engineers will spend a ton of time building custom agents. You know, out of all the things we're going to talk about here, all the big bets I believe top 2% engineers are going to make, I think this is the most important one. Custom agents above all.

All right. Why? It's because one well-crafted agent can replace thousands of hours of manual work. Let me be super clear about this. Right now, there is a custom agent running doing someone's job better than they can. Okay? And it is 50 lines of code, three tools, and maybe 150 line system prompt. All right?

#### The Real Question

Now, the trick is, and we talk about this on the channel, can you build this? Do you know what it takes to build this? Can you prompt engineer? Can you context engineer? Do you know the right model at the cheapest price that can let you build this agent that solves hundreds of hours of work?

#### How Custom Agents Build Trust

All right. When you fine-tune an agent to solve one problem extraordinarily well, your trust in that agent skyrockets. It all comes back to trust. Okay. The the foundational problem, right? The limitation is not the model. It's not the agent, right? The the capability is all there. It is our ability to build the right thing to put together the right context model prompt tools in our agent to increase the trust we have in the tools. Okay, it's all about trust.

Now, with that being said, there are limits to this. There are limits to what you can do with a single agent. So, guess what you can do? You already know what I'm going to say.

---

### Big Bet #4: Multi-Agent Orchestration

The big trend we've been talking about on the channel, we've explored this. We have working versions of this in Agentic Horizon, right? Multi- aent orchestration. This is the new pattern that I'm betting big on and I think other top engineers are looking at this too as the next frontier in 2026. Multi- aent orchestration.

#### The Evolution: One Prompt → One Agent → Many Agents

All right. So in 2023 I said one prompt is not enough. This is before we had any of the clawed models. This is before we had claude code. This is before reasoning. Soon after 01 was released and prompt chaining aka reasoning became the standard. So we were right about that.

All right. We saw this trend coming from miles away. Now, here's the update. One agent is not enough. Stop running a single cloud code instance. Start running 3, 5, 10, hundreds. Multi- aent orchestration is the next big trend. This is what's going to happen next. I'm betting on this.

#### The Math: 1 Agent vs 10 Agents

All right. It's it's for this reason, right? Tell me who wins. A senior engineer with one agent or the top 2% running 10 agents. Assuming we're doing legitimate work with each agent, tell me which engineer wins. Of course, this is a stupid question. The engineer with 10 agents wins. Stack that up over a week, month, and a year. And we're talking magnitudes of of gapping, you know, of of diffing. We're talking a difference of magnitudes in the output from these engineers. Okay? And and this goes crazy once you start adding time as as the variable, right? Weeks, months, quarters, years.

#### More Agents = More Verification = More Trust

Okay, so the trust here is simple and we've explored these patterns in tactical agent coding. This is one of the central thesis of of that course. More agents gives you more verification and that gives you more trust. Right? What one planner or reviewer will miss three will find. Okay? Then you can use powerful techniques like cross validation through multiple agents to directly increase your agentic output to directly increase the trust of your agents.

#### Sub-Agents: The First Step

Okay. So multi-agent orchestration is the next big trend. For instance, running cloud code with sub aents. Why was that release so important? It's because this was the first look at orchestrating multiple agents through an agent. Okay? Because as I've mentioned before on the channel, you prompt your primary agent and your primary agent prompts your sub agents. This is in fact the beginning of multi- aent orchestration. So we saw that all the way back here and in between July and you know the new year in tactical agent coding and agent horizon I've shared private versions of that future right of multi- aent orchestration.

So, this is a big trend.

---

### Big Bet #5: Agent Sandboxes

As you start scaling up your agents, you quickly run into problems with where do you put them? Like, what is the proper space to place your agents? Are are you really letting them all run rampant on your computer? A lot of vibe coders are figuring out that's how you delete your device, right? And a lot of, you know, legitimate mid senior plus level engineers, you know, that there is just not there's not enough tooling and safety and and proper scale to place this many agents right on your device. And and to be clear, you can do that. I I do do that. But you have to be very careful about that. And we can do much better, right?

#### Where Should Agents Run?

Where should all these agents run? They should run in, of course, agent sandboxes. Okay? Let your agent run rampid in its own computer. The top state-of-the-art models by the big labs can do this. Now, okay, how does this tie to trust?

#### Sandboxes = Deferred Trust

High trust agentic engineering sometimes just means having a space where even if everything goes wrong, it doesn't matter. Scrap all of language models. Scrap every agent you've ever seen. Forget about the generative AI revolution. Go back to foundational engineering. This is what the dev box is, right? This is what the staging environment is. We're just preparing and we're building systems of trust that if something goes wrong here, it doesn't matter. Only production matters, right? So, give your agents their own staging environment, right? And really, this is a development environment for each agent.

#### The Pattern: Best of N

Okay? Your agents can run in their own computer, their own sandboxes. And this directly relates to trust because we just defer it. I want to solve this bug. Great. I'm going to spin up 10 agents in their own computer. Everyone gets a shot at this. And you don't care about any one of them except the winner. You defer trust. Okay. Let the agent prove themselves. This is the best of end pattern we've talked about on the channel. This is what you can use for both green field and brownfield code bases.

#### Why Top 2% Engineers Use Sandboxes

All right. Top 2% engineers will parallelize, scale, and isolate green field and brownfield codebases in agent sandboxes. Let the agent run everything end to end. This will help you scale your impact and just straight up defer the trust entirely until you need it. Right? We're basically lazy loading the result. Lazy loading trust in that agent sandbox.

#### The Workflow

Right? when they've completed the work, they've exposed the, you know, port or the result you're looking for for that training run or for that UI change, whatever type of work you're doing. You just pull the result down or you, you know, this is another uh kind of angle on this, or you go into their machine and you look at the result, you see if you like it, and then finally you put up that PR or you say, "Go ahead and merge it." Right? Sandboxes let you defer trust. Okay? You don't need it until the very end when you're merging back to main.

#### It's Just Scaling Up

We're just scaling up what our agents can do, right? This is how you would onboard and and you know train a new engineer on your team to operate. You get your own development environment. Here's how you can run everything. It's a safe place. We're now giving our agents that capability because guess what? No surprise to us on the channel. Our agents are becoming more and more capable.

---

### Big Bet #6: In-Loop vs Out-Loop

All right, big bet number six. Inloop versus outloop. What does this mean?

#### Out-Loop Defined

Outloop is where you prompt through some external system, right? Slack, Discord, GitHub, or your own personal system. Hint, hint, wink, wink. And you let your agentic coding system handle the work. And then they submit a PR or they deliver some concrete result for you, right? Maybe it's a new mock codebase, a prototype, a UI, or most commonly a PR, right? A pull request.

#### In-Loop Defined

And then we have inloop agent decoding. So we have outloop agent decoding and nloop. So inloop is what most engineers are doing. Usually terminalbased agent decoding where you babysit your agent and worked one prompt at a time prompting back and forth and back and forth.

#### The Opportunity

Now this is super powerful but I can guarantee you there are tasks you are doing that you do not need to be doing. Top 2% engineers in 2026 will have both systems progressively offloading more tasks into their outloop agent decoding system.

Okay, this is a massive topic of conversation inside of tactical agent decoding. That's my take on how to build the best systems with agents. In that course, we hint at a lot of the upcoming trends in 2026. We get ahead of all of them. If you want an advantage, check that out. Link in the description.

#### Building Trust in Out-Loop

So, the idea here is that you'll gradually build trust in your Outlook system and you'll hand off more work to your team of agents that have, you know, more agents, different task types, more AI developer workflows, ADWs. And as your trust grows, you offload more work here.

Okay? This frees up your inloop agent coding right when you're spending your precious engineering time and resources in the terminal or in whatever you know tool you like to to use. This frees up your time to really dial into the most important work.

#### The Goal: Maximize Out-Loop, Minimize In-Loop

All right. And the goal here is quite simple. You want to maximize the number of tasks your outloop agent coding system is doing and minimize the end loop.

#### Own Your Agentic System

Now many engineers know by this point there are tons of cloud tools that can help you do this. That is a great place to start. It's not a great place to end. You don't want an external service operating and owning the most important resource an engineer has, their product, their tool, their core source code, right? You want to have your own agentic system, your own agentic ring around your codebase that operates it for you better than any cloud tool can. Okay, this is the central idea inside of tactical agentic coding. Again, link in the description if you're interested in checking that out.

---

### Big Bet #7: Agentic Coding 2.0

Big bet number seven, Agentic Coding 2.0.

#### The Evolution Timeline

Last year I made a bet that we would see a new paradigm emerge. In 2024, AI coding emerged. We were using tools like Klein, like Ader. We were just writing code, right? Context model prompt big three. In 2025, cloud code brought us a gentic coding agents that can engineer. They're calling tools, right? Again, this is the core four context, model, prompt, tool. Everything is that. I'm going to say that over and over again on the channel so it's crystal clear to you. I don't want you to get baited by any, you know, feature coming out by any lab or, you know, tool. If you understand that fundamental truth that everything is just the core 4, you will be able to build and operate at the highest possible level.

#### What's Next: Agents Conducting Agents

All right. Now, I'm predicting in 2026, top 2% engineers will push into the next frontier, the next level of agent coding. I'm calling it agent coding 2.0. You can call it whatever you want, but the idea here ties right into our multi- aent orchestration. Agents that conduct agents.

#### The Lead Agent Model

The idea is very simple. You are now talking to your lead agent. Your lead agent is then spinning up the right agent for the job, right? Multi- aent orchestration. This is not parallelization. Let's be super clear. That's a part of it. But this is true orchestration. Okay? You have a lead agent that can delegate, monitor, and coordinate.

#### Command Level Agents

I call them command level agents, but worker agents is another term. You can call these whatever you want, but your lower level agents. If you're in the cloud code ecosystem right now, these are just called sub agents. This is a great place to start, but there's more work that needs to be done on the coordination level and the specialization of your lead agent, understanding that it can effectively crud and spawn and delete other agents.

#### The Orchestrator's Job

All right, so your orchestrator plans, spawns, reviews, and ships. Okay. And this is interesting cuz once again, we're seeing this trend where we move up the stack. Again, we're not even the lead engineer. We're the executive, right? Because we're just talking to that lead engineer that runs the engineering team. Okay? And we're prompting them and they're taking care of the rest. Very interesting paradigm.

#### Why This Bet?

Okay? So, I'm betting on this. Why is that? It's for the same reason that multi-agent orchestration is so powerful. More agents gives you more compute, gives you more trust. Your lead agent understands it, can control other agents. doesn't solve your problem. You stop talking to individual agents and start talking to your lead agent that conducts the orchestra. You trust the conductor will deliver the job for you, right? You trust the system that you've built.

#### Putting It All Together

And again, this really hints on multiple ideas we've been building up to. Right? Now, we have tool calling on top of custom agents on top of multi-agent orchestration on top of our outloop systems and our inloop systems when we need to jump in. Right? all going to turn into this big this big system, right? Some some like to call this the swarm. I don't think it's going to be that big or that, you know, novel. I think it's just going to be this pattern. You lead agent command level agents.

#### The Orchestrator Is a Custom Agent

Okay? And you might think that it's just a primary agent and sub agents. But that's not it. It's agent orchestration. You need to teach this agent inside of Agentic Horizon. In that multi- aent system, the orchestrator agent aka the lead agent, it itself is in fact a custom agent, which just hints at again why custom agents are so powerful. You can build any type of agent to solve any problem through tool calling.

---

### Big Bet #8: Benchmark Breakdown

All right, let's move on. Big bet number eight, benchmark breakdown.

#### The Saturation Problem

So, here is a super super mega benchmark that proves that the model is the best at everything. And you can see all the models are at 100%. This is in the future, okay? This is going to happen. And you know, this isn't the bet itself, right? The bet here is that public benchmarks get saturated. Every model hits 90 to 100%. They no longer truly give you or I signal.

#### Models Prove Themselves to YOU

The models don't prove themselves on these benchmarks. They prove themselves to you doing legitimate engineering work or solving whatever problem your user has that you solve for them.

#### The Solution: Private Evaluations

All right. So, what are top engineers doing here? They're going to build private evaluation systems that will never become public. Okay? And a lot of companies, a lot of labs, they already do this, right? There are benchmarks they will never share. And this gives them a massive edge.

#### Build Your Own Benchmarks

And so I think if you're building a company, if you're building custom agents that actually matter, that do real userf facing work, you will measure it yourself. Not what others tell you, not what any other benchmark says.

#### The Cost of Not Having Private Benchmarks

you know, there is very likely a model out there right now that can do the job of a custom agent that you're running and you just have no idea because you do not have benchmark. So, this is harder. You're likely paying more for an expensive model, but also it means that when a new model comes out that you should use to get incredible results for your users, you won't even know it's there because you have no evaluations. You have no private benchmarks.

#### The Big Bet

The big bet here is quite simple. It's a continuation bet that top engineers will always have private benchmarks to measure the models for the use case they care about.

---

### Big Bet #9: UIs vs Agents

Moving on, UIs versus agents. This is super interesting. Um, I'm betting that agents are eating SAS.

#### The Existential Threat to SaaS

And I think we're already starting to see this. Every software as a service company that doesn't start eating themselves with agents will be eaten by a agent focused, agent forward tool. Okay?

#### The Google Search Example

And we see this at large with Google. The Google search bar is a simple kind of really clear example. Why would I search and get a list of 10 items when my agent can do this at absurd scale and speed?

#### Agents Are The Interface

So top engineers will realize agents are the interface. Many UIs become prompt interfaces that will look like chatbt. I don't like how overused the chat interface is. I've been saying this for over a year. I would love to see new UI patterns emerge and I think this bet plays on that.

#### What's Behind Modern UIs

Okay, a lot of the best UIs are going to run doubledigit number of agents behind the scenes to accomplish highly specific tasks better than any raw deterministic code could.

#### Why Top Engineers Choose Agents Over UIs

Top 2% engineers are going to stop using these slow SAS applications where you have to click through these slow UIs to do one thing that ultimately is just crud on top of a database. Okay, top engineers are going to take agents and just build the minimum tooling. Not even an MCP server, right? They're going to take tools from the MCP server, chop them down, and optimize tool calling so that they get full control over interacting with that service at light speeded. And then you can push that idea into products.

#### The Vision

Why do I have to operate this UI? Why can't I just ask my computer what I want to accomplish? And that is the UI. Okay. So, I think we're going to see this interesting UI versus agents battle happen in 2026 and top engineers will be on the side of the agents, right? Because that gives you once again trust, speed, iteration, and impact right inside the UI. Okay, so that's the idea there.

---

### Big Bet #10: AGI Hype Dies

And let's move on to our final bet here. I believe that the AGI hype dies.

#### Focus on Agents, Not AGI

Okay, and we start really locking into just the thing that matters, which is agents. Okay, as mentioned in the beginning, you know, Andrew Karpathy, he says, "This is the decade of agents." I completely agree with that. I think agents are the gift and frankly one of the final stages of language models.

#### AGI/ASI: The Greatest Marketing Scheme

Now, not to say that there aren't more compositions to be had, but this whole AGI ASI stuff, they're not even close. It's one of the greatest marketing schemes of all time. I am a massive bear on this. I I have never cared about this stuff. It's interesting. It's novel. It sells very well, right? It it shows up in headlines really well. But time and time again, we've seen that this is just factually untrue and it's been used as a marketing scheme.

#### Ship, Don't Dream

Top 2% engineers stop caring about AGI promises and just lock into delivering maximum value with agents. Okay, let's just kill the AGI hype, right? Let's let's focus on performing and controlling the things that we can, which is the best tool of engineering, agents.

Okay, um I'm just fully sick of this. So, and so I'm betting that other engineers are really catching on to this and and we'll just stop caring, right? Like I I don't respond to, in fact, I'm detracted from, you know, this AGI ASI hype and talk and it's a great north star for the deep researchers inside of these AI labs, but for everyone else, it's a complete waste of time. It's vaporware. Okay, let's focus on shipping useful software.

---

## Part 3: Bonus & Retrospective

### Bonus Bet #11: End-to-End Agentic Engineers

All right. So, for everyone that made it here, um, you know, I know this is like kind of a deep presentational video with a lot of like conceptual ideas that if you're not engineering with this stuff, probably doesn't make any sense, but um, I have a bonus bet for you here, right? Number 11. So, this is a hidden bet. If you just skip through this video, I'm hoping that you miss this, right? I I I really want to reward engineers that are really committed and in planning for themselves in the future and you want to, you know, see what other engineers are planning to improve what they can do. That's what this is all about, right? Top 2%, top five, top 10, whatever you want to, however you want to frame that, right? I think top 2% is a great number. That's the elite of the elite. And then we can go from there, right?

#### The Edge Bet

Big bet number 11. We see the first endtoend agentic engineers emerge. Now, this is a super edge bet. I'm probably going to get this wrong, but I'm betting on this.

#### What This Looks Like

We're going to see the first engineer write a blog post that builds a complete system that builds the system. So, so this is this is the this is a central idea we talk about in tactical agents coding, right? You you don't want to focus on building the application anymore. You want to focus on building the system that builds the system, right? The system of agents that does the work for you.

#### Prompt to Production, No Human in Loop

So, here's what we're going to see. the first blog post where an engineer shares, you know, some details about their chain of agents that ship an entire feature end to end. And I'm talking end to end, prompt to production, no review, no human in the loop.

This is going to be the pure polar opposite of those stupid outdated why can't AI engineer blogs and just like the the anti- AI crew and engineers who are just going to get smoked every single day moving forward from now, right? And they already have been, frankly.

#### The Reaction

So end to end agentic engineers start to emerge. It's going to freak people out. But you know you and I and longtime viewers of the channel we've seen this coming right. In fact once again it it is inside tactical agent coding. This is an idea we talk about in some of the final lessons. This is coming to engineering.

#### The Ultimate Expression of Trust

Okay. And in that framing of trust if you think about it this is the highest possible level of trust we can have in our agents. This is the north star for trusting your agents. Okay. And in a lot of ways, this is the endgame of agentic engineering. It's full autonomy, prompt to production, no human in the loop. The ultimate expression of trust in your systems. Just like, you know, when your manager hands off a feature, a a road map to the team and then they just disappear. They know the best teams always ship.

---

### Summary: The Year of Trust

Those are the big bets. Okay. So, this is the year of trust. Everything I'm doing and everything that I believe top 2% engineers will be doing is centered around increasing the trust they have in their agentic systems.

#### All Bets Recap

Right? All the way from betting on the right labs, increasing their tool calling, building out custom agents to solve custom problems to make a custom custom ton of money, right? And then you start scaling that up. you start building out an orchestration agent that can then drive these results that you're looking for.

All right, once you have multi- aent orchestration, you're going to want to put all of your agents somewhere safe where you don't have to care about when things go wrong until you want to pick that result, right? Best of N.

You're not going to want to sit in the terminal all the time. It's too slow and there are problems you just don't need to solve in the loop, prompting back and forth, babysitting your agent. This is why you're going to want to build up an outloop system.

All right. I think when you put together these previous five points, you get agentic coding 2.0. You get to talk to a lead engineer that understands your problem, your codebase, everything you're trying to do with a bunch of experts aiding it. And then that agent spins up the right command level agents, right? The right specialized agent with their own system prompt, right? their own specific playbook on how they should best plan, build, review, test, and document that specific work, right? That all happens in a brand new agent coding 2.0 user interface, uh, application, product. I'm not sure what this looks like, but I'm predicting this emerges.

We have you prompting a lead agent and the lead agent commanding one or more agents to accomplish the work, specializing every agent down to the system prompt, down to the user prompt.

#### Benchmark and UI Predictions

All right, we we're going to have a benchmark breakdown. Top engineers in this year of trust, the trust disappears in these public benchmarks. I think we're already starting to see this. Top engineers just like you and I I have this experience, too. I look at some of these benchmarks and I go, great. Uh, you know, I'm going to block out some time. I'm going to run some benchmarks I've created and I'm going to really get a feel for this for use cases I care about.

All right, we're going to see UI versus agents. Agents are going to eat up UIs. You're going to eat up SAS applications. If your application is just a UI on the database, expect it to be cooked. So, start building agents on top of it to automatically and agentically operate that database for you and for your users.

#### AGI Conclusion

And you know, number 10, AGI hype dies. We lose trust in these labs selling this vision of AGI and ASI and we just focus on shipping.

---

### 2025 Predictions Review

Okay, so here's what I'm betting my time, money, and compute on. I would love to know what is your plan for 2026. What do you agree with? What do you disagree with? Comment down below and let me know.

Quickly taking a look at the 2025 prediction. So, you know, here's all the the 15 bets I made.

#### Bet 1: AI Coding Becomes Standard ✅

Um, AI coding becomes a standard. This is true. 84% of developers now use AI coding tools. All right, adoption is not universal. It's funny to think back at the beginning of last year. There were still engineers saying things like LLMs could never write code. They could never do XYZ. Right? Again, this is before Cloud Code, Codeex, Gemini, before all of that. Right? The first reasoning model had just dropped when I was making this prediction. Okay? So, we got that right.

#### Bet 2: Agentic Coding Begins ✅

Agentic coding begins. I predicted that our tools that were just coding would start calling tools reliably. Okay? And that's the crux of that. If it gets one out of four, right, it doesn't matter. That's not good enough. But claude code was the primary driver. Then we got cursor, agent mode, codec, gemini, everyone followed. Right. This was true. Nailed that.

#### Bet 3: Evergreen AI Coding Experience ✅

Right. Evergreen AI coding experience. Again, we have to give credit to cloud code. It redefined agentic coding. Surprisingly, it was in the terminal. Um, I was expecting this to be a UI, but it was in the CLI, and this worked out better than anything. So we got a brand new AI coding experience in a way. Number two and number three are very interlin right agent coding did begin and a new coding experience to begin inside the terminal.

#### Bet 4: Cost of Code Declines ✅

All right. Uh the cost of code declines. Um not a huge bet to make but this has remained true and we're seeing that with the new Gemini 3 flash release. The cost of code is barreling towards zero. To be clear, the right configuration of code in a product is still as valuable as it was before. In fact, it's more valuable because there's a lot more code out there that is absolute trash and slump.

#### Bet 5: Skill Gap Earthquake ✅

Skill gap earthquake 25% decrease in entry-level roles. Senior AI orchestrators salaries skyrocketed. I got this one right. We see a massive decline in entry-le roles. This is still continuing throughout the entire year. Uh this has been proven true. We get points for that looking back.

#### Bet 6: No LLM Wall ✅

All right. LLM model review. So, there is no wall. Again, if you remember at the end of last year, the end of 2024 when I was making this bet, it was very unclear whether uh models could push past where they were. I bet super hard that they could. Again, I'm going to link my previous predictions video. It was a very different world one year ago in December, right? 2024 December was a very different world. Everyone was talking about the wall, right? I was right. There is no wall. Benchmarks have been destroyed. New benchmarks have been introduced. And guess what's going to happen? Those are going to get destroyed. I didn't make this a bet for this year because I think the writing's on the wall. There is no wall.

#### Bet 7: Hyperspecialized LLMs ✅

Hyperspecialized LLMs got this one right as well. We see LLMs, even the best ones, start to specialize, right? Cloud opus is for engineering. We got this new function Gemma that was just released specifically for tool calling on devices, right? Code stroll. Uh we got enterprise models. We got chemistrybased models. This is going to continue. Um, again, a lot of these bets become pointless to continue to make because it's just so clear now in the industry that there's no alpha here anymore, right? There's no alpha in betting in this. All right, there's no advantage to be had. Okay,

#### Bet 8: Year of Infinite Memory ❌

year of infinite memory. I got this one wrong. I got this one wrong. I did not understand this problem deeply enough. Context kind of grew. And specifically by grow, I mean the effective context window of a lot of these models did improve, but memory is still a huge challenge. I got this wrong.

#### Bet 9: Small Language Models on Devices ✅

All right, we saw GPT4 Ole small language models on devices. This is true 03, Haiku, Quinn. There's a bunch of models now that can run on a really shitty device all the way down to mobile devices. All right, so I got this one right. These are probably not the right models to list here, but um you can tell I had an agent spin that description out for me.

#### Bet 10: OpenAI & Anthropic Release Small Models ✅

I predicted uh OpenAI and Antropic would release small language models. Got this one right. We have Haiku and we have uh 03 but also we got the GPT OS models right even smaller 120 billion and 20 billion.

#### Bet 11: OpenAI Remains #1 ❌

All right, OpenAI remains number one. I got this prediction wrong. They are no longer in the top place. Right now it's probably between Opus 5 or Gemini 3 in terms of the best all-around model for price um intelligence and speed. Very clearly the winner right now is Gemini 3 Flash as we talked about in the beginning.

#### Bet 12: New Industry-Breaking Architecture ✅

Uh new industry-breaking architecture. We now have world models and this slowly is moving outside of the realm of engineering for us. We probably won't be covering or talking about these a lot unless there's some serious advantage for us for modernday uh technology and modern day software engineering. But certainly we could be. I predicted this. It came true. We we now have this new world of world models.

#### Bet 13: Exponential Slop ✅

Exponential slop. This came true. This prediction describes itself.

#### Bet 14: Big Tech Shrinks, SMBs Grow ✅

Uh, big tech shrinks, smallmedium businesses grow. I think this one's a little more of a mixed bag, but ultimately this one's true. You could really frame this as correct or false if you wanted. I'm going to take the credit here. We did concretely see big tech cut jobs while small businesses were able to thrive with smaller teams. Uh, this one's a little weird. If you want to dock me on this one, you can. I totally understand that.

#### Bet 15: Data > UX > Benchmarks ✅

Last last bet here that I made last year. Data greater than UX, greater than benchmarks. Winners have proprietary data and better UX and the raw model intelligence was commoditized. I think this is right and it will continue to be right. The most important thing now is your user data and the relationship you have with your user and access to that data. Then it's your user experience. And then lastly, it's the language model benchmarks and the language model themselves that use to feel your your new agentic experiences.

---

## Closing

### Call to Action

All right. So, these are all the actions I'm taking for 2026. I believe this is going to be the year of trust. Once again, this is where I'm betting my time, money, and compute. Let me know what you think in the comments. Do you agree? Do you disagree? What am I missing here?

I'm betting super big on these ideas. I have many of these ideas already in motion. Again, as mentioned, if you're interested, check out Tactical Agentic Coding link in the description. Many engineers who watch the channel are already in there. You're already in Agentic Horizon. you're already getting the edge on 2026. That's going to be in the description if you're interested.

### The One Bet to Rule Them All

The big idea here, if I had to pick one, it's this custom agents above all when it comes to signal, when it comes to value, I think this is the one thing you can bet big on and you'll see the highest return on investment. After that, right, once you're building out custom agents, you want to start pushing into multi- aent orchestration, agent sandboxes, and this will naturally kind of get you into this next state.

### Final Mindset

You want to have that mindset of models don't matter anymore. There's no AGI, there's no ASI, it's just agents. So, let's put my team of agents together. Let's build the best custom agents. Let's build a great lead engineer that helps me communicate with my agents. And let's ship. All right? Let's get real work done.

That's what this channel is and will always be about. You know where to find me every single Monday. Happy holidays. Stay focused and keep building.
