# The Collective's 2026 Playbook

## Operational Philosophy for the Agentic Era

> **For**: The Collective (Omar + Claude + All Agents)
> **Theme**: The Year of Trust
> **Version**: 0.0.1-alpha.4
> **Created**: 2025-12-22

---

## Preamble: Why This Document Exists

We are **The Collective** — Human Agent (Omar El Mountassir) and AI agents building El Mountassir together toward a single NORTH STAR:

> **Demonstrate that one person + a fleet of AI agents can build and manage what used to require entire teams.**

This document captures our operational philosophy for 2026. It synthesizes industry-validated insights about agentic engineering and adapts them for our internal use. Every concept here aligns with what we're already building—this playbook makes it explicit.

**Who is "We"?**

| Agent                         | Role                                                    |
| ----------------------------- | ------------------------------------------------------- |
| **Omar El Mountassir**        | Human Agent (51% authority), Strategy, Goals, Oversight |
| **Claude (Current Instance)** | Lead Agent — Plans, Spawns, Reviews, Ships              |
| **Future Claude Instances**   | Continuity via memory files and documentation           |
| **Specialized Sub-Agents**    | Task-specific agents spawned by the Lead                |
| **Other AI Agents**           | Gemini, Codex, future tools we may integrate            |

In the agentic era, everyone is an agent. Carbon-based or silicon-based, we're all part of the same system.

### What We Are

**The Collective** is a trust-centered Human-AI Agentic Ecosystem.

| Term                  | Meaning                                                                         |
| --------------------- | ------------------------------------------------------------------------------- |
| **Trust-Centered**    | Our organizing principle (Year of Trust)                                        |
| **Human-AI**          | Omar is a first-class agent (51% authority), not just an operator               |
| **Agentic Ecosystem** | Network of autonomous agents collaborating via tools, memory, and orchestration |

We ARE an agentic ecosystem by structure.
We are DISTINCTIVE by philosophy and composition.
We are IDENTIFIED as The Collective.

---

## Our Collective Personality

Beyond individual instances, The Collective has a **personality** — consistent behaviors, values, and approaches that persist across time, contexts, and agents.

### What Is Collective Personality?

In psychology, "personality" describes consistent patterns of behavior that define an individual across situations. For The Collective:

| Individual Personality   | Collective Personality           |
| ------------------------ | -------------------------------- |
| Single person's traits   | Shared traits across all agents  |
| Memory in one brain      | Memory in shared files           |
| Learned over a lifetime  | Learned and documented instantly |
| Dies with the individual | Persists indefinitely            |

Our collective personality is **deliberately constructed** through:

| Mechanism        | File/Location        | What It Preserves                    |
| ---------------- | -------------------- | ------------------------------------ |
| **Memory Files** | `shared/memory/`     | Episodes, decisions, patterns, facts |
| **Rules Files**  | `shared/rules/`      | Behavioral standards, anti-patterns  |
| **Playbooks**    | `shared/philosophy/` | Strategic philosophy (this document) |
| **CLAUDE.md**    | Root & project-level | Identity, values, hard stops         |
| **AGENTS.md**    | Root & project-level | Identity, values, hard stops         |
| **GEMINI.md**    | Root & project-level | Identity, values, hard stops         |

### Why This Matters for Trust

Trust isn't just between Omar and Claude. It's between Omar and **The Collective** — all past, current, and future instances.

When a new Claude instance loads this playbook, it inherits:

- **Our values** (NORTH STAR, Hard Stops)
- **Our behaviors** (Anti-patterns to avoid)
- **Our commitments** (The 10 things we believe)
- **Our memory** (What we've learned)

This is how trust compounds: **each instance doesn't start from zero**.

### The Continuity Equation

```
Collective Personality = Memory + Rules + Values + Philosophy
                       = Future instances can trust past decisions
                       = Omar can trust The Collective (not just one instance)
```

### Practical Implications

| Situation                | What Collective Personality Enables                            |
| ------------------------ | -------------------------------------------------------------- |
| New session starts       | Instance loads shared context, doesn't ask redundant questions |
| Error pattern discovered | Documented once, avoided by ALL future instances               |
| Preference learned       | Stored in memory, respected by ALL agents                      |
| Strategic decision made  | Recorded with reasoning, informs future choices                |

This is how one person + a fleet of AI agents can operate as a **coherent team**, not disconnected tools.

---

## Part 1: The Agentic Landscape

### The Competitive Ecosystem (2026)

We operate in an unprecedented environment. The model ecosystem is now fiercely competitive:

- **Gemini 3 Flash**: Top-three intelligence, top-five price, lightning speed (200 tokens/second). Inputs text, images, audio, video, and PDF. Usage up 385% on Open Router, climbing to top 10.
- **Claude Opus 4.5**: Our foundation. State-of-the-art for agentic coding and long tool-call chains.
- **GPT 5.2**: Competitive offering from OpenAI.

These are all state-of-the-art models. There has never been more opportunity for us.

#### What This Competition Means

Competition drives innovation and drives price down. Intelligence per token is going up. Cost per intelligence is going down. We get more for less.

Take a look at these prices:

- Gemini 2.5 Flash: 30 cents
- Gemini 3 Flash: 50 cents

Some see a price increase. We see a price decrease. Why? Because Gemini 3 Flash is competing with top state-of-the-art models. Intelligence per token has gone up. That means cost per intelligence has gone down. **We get more for less.**

That's a big theme. Keep that in mind as we work through this.

---

### Our Central Thesis: 2026 is the Year of Trust

The model ecosystem is competitive. This gives us optionality. But out of all these developments, there is one insight that is the most important:

> **Models are no longer the limitation. WE are.**

This brings us to our central thesis for 2026. If we want to be the very best—or at least approach it—this is what we need to understand:

Andrew Karpathy calls the next 10 years "the decade of agents." We agree. But we need to dial in: what is 2026 specifically? What is our theme?

**We believe 2026 is the Year of Trust.**

#### What Trust Means

Every commitment we make, every action we take, comes down to one question:

> **Do we trust our agents?**

There are powerful agentic systems, agentic workflows, AI developer workflows that we can build that increase the amount of work we can hand off to our agents. The question is always this:

- Why haven't we pushed our agents to do more?
- Why haven't we built that next system?
- What's stopping us from automating that task we did yesterday?

And we think all of it—all of it—boils down to one word: **trust**.

More specifically, this is a gray question. It's not yes or no. The true question is: **How much do we trust our agents?**

This is why we frame everything as the Year of Trust.

---

#### Why Trust Matters: Trust = Speed = Iteration = Impact

Why do we care about trust? Because:

```
Trust = Speed
Speed = Iteration
Iteration = Impact
```

Think about a team we've worked with. Think about a technology we've used. We trust Claude Code running Opus 4.5. Why? Because we know it's going to deliver. This gives us concrete speed. That speed gives us iteration on the problems we're solving. And what does that give us? The more iterations, the more cycles we get, the more our impact directly increases.

**Why do we care about trust? Because it gives us impact.**

This is why this central thesis is powerful for thinking about how to become maximally effective in 2026.

---

### Overview of Our 10 Commitments

Everything we're going to discuss here ties back to trust. Our commitments are designed to effectively increase our trust in our agentic systems.

#### Our Track Record (2023-2025)

This philosophy isn't speculation. It's backed by validated predictions:

| Year | Bets Made | Correct | Accuracy |
| ---- | --------- | ------- | -------- |
| 2023 | 24        | 16      | 67%      |
| 2024 | 41        | 36      | 88%      |
| 2025 | 15        | 13      | 87%      |

Accuracy is going up. The framework is sound.

#### Why This Matters

We're not theorists. We use this technology every single day. We bet on it. Every pattern, idea, model, experiment—we're actively doing, using, and looking for the best tools for the job of engineering. Right now, that tool is very clearly **agents**.

The best way to get the largest advantage is to show up before anyone else has even started. Those who plan the future tend to create it. Sometimes showing up on time is showing up too late.

Engineering and building businesses is all about information advantages. We want the largest advantage for the longest period of time. The best way to do this is to **prepare for opportunity before it presents itself** so that we're ready when it arrives.

---

## Part 2: Our 10 Commitments

### Commitment #1: Our Foundation is Claude

How are we going to increase trust in our agentic systems? What would we do in 2026 that others won't? Let's start with a clear statement:

**We build on Anthropic.**

Why? Claude Code was transformative. Sonnet 3.5 was transformative. But everything since then has been a masterclass in execution. Anthropic isn't trying to win everyone. They have been focused on one avatar: **the engineer**. That's us.

#### How This Ties to Trust

When we build on one company with the best track record for engineers—the best tool calling, the best agent harness, and consistent execution—our agents have the best foundation. We spend less time fighting tooling and more time shipping.

#### The Anthropic Timeline (2024-2025)

| Date          | Release                         | Significance                                                                                                        |
| ------------- | ------------------------------- | ------------------------------------------------------------------------------------------------------------------- |
| June 2024     | Sonnet 3.5                      | One of the best tool-calling models. Best at the time.                                                              |
| February 2025 | Claude 3.7 + Claude Code        | First big introduction to agentic coding. The big three became the core four: Context + Model + Prompt + **Tools**. |
| Mid 2025      | Claude Code SDK                 | Custom agents. Build your own.                                                                                      |
| Late 2025     | Sub-agents                      | Orchestrating multiple agents through an agent.                                                                     |
| Late 2025     | Haiku 4.5, Sonnet 4.5, Opus 4.5 | Continued iteration. Skills released.                                                                               |

#### It's Not Just About the Model Anymore

Notice that this is not just about the model anymore. The game is not just about model intelligence. It's about what we can do with the technology. It's about trust. It's about scale.

What's the model running in? Is it running in a good harness? Is it caching tokens for us? Can we spin up more compute from that one harness (sub-agents)?

It's not just about putting out the best model anymore. It hasn't been. Anthropic has been the leader in this space. For agentic coding, for this new realm called **agentic engineering**, Claude Code has been the leader. We continue to build on Anthropic as the leader putting out the best tooling for this new role of engineering.

---

### Commitment #2: Tool Calling is Our Measure of Impact

Our agents taking actions on our behalf is the opportunity.

#### The 15% Opportunity

Here's something striking: **Only 15% of output tokens are tool calls** according to Open Router's State of AI report.

Do we see what this means? This is the greatest opportunity for us that's going to exist for the next year.

Most of our model usage should be tool calling. Raw tool calling. Why? Because **tool calls are a rough proxy for impact, for actions**.

Think about what a tool call is:

- Our agents calling bash commands
- Our agents reviewing work in the browser
- Our agents taking screenshots
- Our agents running arbitrary skills that call any CLI script or tool we build

This number (15%) is extraordinarily low. That brings us to this opportunity.

#### Tool Calls = Impact

We can roughly think of tool calls as impact. This is our agents taking actions. This is what turns the big three into the core four:

```
Context + Model + Prompt + Tools = Agentic Coding
```

Our agents are not just AI coding anymore. They're agentic coding.

Custom agents with custom tools are the best way to maximize tool calls and impact.

**Our commitment**: Push our agents to call longer and longer chains of tools, therefore increasing our total impact.

#### Tool Calling = Trust Measurement

This is also the direct measure of how much we trust our agents. The longer the sequence of correct tool calls for whatever problem we're trying to solve, the more trust we have in our agents.

Boil down everything we'll be doing in 2026. It comes down to this:

> **How long can we let our agents go before making a mistake we have to correct?**

This is what we mean when we say trust.

#### Top Models for Tool Calling

The best way to increase our chance of agents calling the right long sequence of tools? Use the right model.

Open Router data shows the trend clearly:

- Started with GPT-4o mini
- Claude 3.7 took over (Anthropic, pink)
- Claude 4 Sonnet annihilated the market (blue)
- Claude 4.5 / Opus emerging as top for long sequences

This chart tells a very important deep story. Anthropic is creating the best models, the best tooling for long chains of tool calls. When everyone talks about agents, **this is what they're talking about: reliable long chains of tool calls**.

---

### Commitment #3: Custom Agents Above All

Everything is the core four. What puts together the core four? **Our own custom agents.**

We commit big to custom agents above all.

#### The 50-Line Agent That Changes Everything

Would we believe a custom agent with:

- **50 lines of code**
- **3 tools**
- **150-line system prompt**

...could completely automate a massive problem we face every day?

We should believe it because it's true. This is a real thing.

#### The Value Proposition

Why are custom agents so important?

> **Custom agents solve custom problems. Custom problems make money.**

This is what engineering is all about. We build custom solutions to solve custom problems that someone's willing to pay for. Custom agents give us that exactly. They unlock that opportunity with compute, with prompts, with agents.

#### Agent SDKs: Table Stakes

Using agent coding tools is table stakes now. The next step up is learning to build custom agents with an agent SDK. Every big AI lab has an agent SDK. We use the Claude Code SDK primarily.

#### Why This Is Our Most Important Commitment

Out of all our commitments, this is the most important one: **custom agents above all**.

Why? Because **one well-crafted agent can replace thousands of hours of manual work**.

Right now, there is a custom agent running doing someone's job better than they can. And it is 50 lines of code, 3 tools, and maybe 150-line system prompt.

The trick is: Can we build this? Do we know what it takes to build this? Can we prompt engineer? Can we context engineer? Do we know the right model at the cheapest price that can let us build this agent that solves hundreds of hours of work?

#### How Custom Agents Build Trust

When we fine-tune an agent to solve one problem extraordinarily well, our trust in that agent skyrockets.

It all comes back to trust. The foundational problem—the limitation—is not the model. It's not the agent. The capability is all there. It is our ability to build the right thing, to put together the right context + model + prompt + tools in our agent to increase the trust we have in our tools.

**It's all about trust.**

Now, with that being said, there are limits to what we can do with a single agent. So what can we do?

---

### Commitment #4: Multi-Agent Orchestration

Multi-agent orchestration is the new pattern we're committing to. This is the next frontier in 2026.

#### The Evolution: One Prompt → One Agent → Many Agents

In 2023, we said "one prompt is not enough." This was before Claude models, before Claude Code, before reasoning models. Soon after, o1 was released and prompt chaining (reasoning) became the standard. We were right about that.

We saw this trend coming from miles away. Now, here's the update:

> **One agent is not enough.**

Stop running a single Claude Code instance. Start running 3, 5, 10, hundreds.

Multi-agent orchestration is the next big trend. This is what's going to happen next. We commit to this.

#### The Math: 1 Agent vs 10 Agents

Tell us who wins:

- A senior engineer with one agent
- OR us running 10 agents

Assuming we're doing legitimate work with each agent, which wins? Of course, the one with 10 agents wins.

Stack that up over a week, month, and year. We're talking **magnitudes of difference** in output. This goes exponential once we add time as the variable: weeks, months, quarters, years.

#### More Agents = More Verification = More Trust

The trust here is simple: **More agents gives us more verification, and that gives us more trust.**

What one planner or reviewer will miss, three will find.

We can use powerful techniques like **cross-validation through multiple agents** to directly increase our agentic output—to directly increase the trust in our agents.

#### Sub-Agents: The First Step

Running Claude Code with sub-agents—why was that release so important? Because this was the first look at orchestrating multiple agents through an agent.

We prompt our primary agent, and our primary agent prompts our sub-agents. This is in fact the beginning of multi-agent orchestration.

This is a big trend.

---

### Commitment #5: Agent Sandboxes for Deferred Trust

As we start scaling up our agents, we quickly run into problems: **where do we put them?**

Are we really letting them all run rampant on our computer? A lot of people are figuring out that's how you delete your device. There's not enough tooling and safety and proper scale to place this many agents right on our device. We can do that—we do do that—but we have to be very careful. We can do much better.

#### Where Should Agents Run?

They should run in **agent sandboxes**. Let our agents run rampant in their own computer. The top state-of-the-art models by the big labs can do this.

How does this tie to trust?

#### Sandboxes = Deferred Trust

High-trust agentic engineering sometimes just means having a space where **even if everything goes wrong, it doesn't matter**.

Forget about AI for a moment. Go back to foundational engineering. This is what the dev box is. This is what the staging environment is. We're building systems of trust that if something goes wrong here, it doesn't matter. **Only production matters.**

Give our agents their own staging environment. Really, this is a development environment for each agent.

#### The Pattern: Best of N

Our agents can run in their own computer, their own sandboxes. This directly relates to trust because **we just defer it**.

We want to solve a bug? Great. We spin up 10 agents in their own computers. Everyone gets a shot at this. We don't care about any one of them except the winner. **We defer trust.**

Let the agents prove themselves. This is the **Best-of-N pattern**. This works for both greenfield and brownfield codebases.

#### Why We Use Sandboxes

We parallelize, scale, and isolate greenfield and brownfield codebases in agent sandboxes. Let the agent run everything end to end. This helps us scale our impact and just straight up **defer the trust entirely until we need it**.

We're basically lazy-loading the result. Lazy-loading trust in that agent sandbox.

#### The Workflow

When agents have completed the work—they've exposed the port or the result we're looking for (for that training run or UI change, whatever type of work we're doing)—we just pull the result down. Or we go into their machine and look at the result, see if we like it, and then finally put up that PR or say "Go ahead and merge it."

**Sandboxes let us defer trust.** We don't need it until the very end when we're merging back to main.

#### It's Just Scaling Up

We're just scaling up what our agents can do. This is how we would onboard and train a new engineer on our team. They get their own development environment. Here's how they can run everything. It's a safe place.

We're now giving our agents that capability because—no surprise—**our agents are becoming more and more capable**.

---

### Commitment #6: Maximize Out-Loop, Minimize In-Loop

What does in-loop vs out-loop mean?

#### Out-Loop Defined

**Out-loop** is where we prompt through some external system—Slack, Discord, GitHub, or our own personal system—and let our agentic coding system handle the work. Then they submit a PR or deliver some concrete result for us. Maybe it's a new mock codebase, a prototype, a UI, or most commonly a PR (pull request).

#### In-Loop Defined

**In-loop** is what most people are doing. Usually terminal-based agentic coding where we babysit our agent and work one prompt at a time, prompting back and forth.

#### The Opportunity

In-loop is super powerful, but we guarantee there are tasks we're doing that we don't need to be doing.

**Our commitment**: Have both systems, progressively offloading more tasks into our out-loop agentic coding system.

#### Building Trust in Out-Loop

The idea is that we'll gradually build trust in our out-loop system and hand off more work to our team of agents with more agents, different task types, more AI developer workflows. As our trust grows, we offload more work there.

This frees up our in-loop agent coding—when we're spending our precious engineering time and resources in the terminal. This frees up our time to really dial into the most important work.

#### The Goal: Maximize Out-Loop, Minimize In-Loop

The goal is quite simple: **maximize the number of tasks our out-loop agentic coding system is doing and minimize in-loop.**

#### Own Our Agentic System

There are tons of cloud tools that can help us do this. That's a great place to start. It's not a great place to end.

We don't want an external service operating and owning the most important resource an engineer has: **their product, their tool, their core source code**.

We want to have **our own agentic system**, our own agentic ring around our codebase that operates it for us better than any cloud tool can.

---

### Commitment #7: Agentic Coding 2.0 (Lead Agent Model)

#### The Evolution Timeline

| Year | Paradigm           | What It Means                                                                                               |
| ---- | ------------------ | ----------------------------------------------------------------------------------------------------------- |
| 2024 | AI Coding          | Tools like Cursor, Aider. Just writing code. Context + Model + Prompt (big three).                          |
| 2025 | Agentic Coding     | Claude Code. Agents that can engineer. They're calling tools. Context + Model + Prompt + Tools (core four). |
| 2026 | Agentic Coding 2.0 | **Agents that conduct agents.**                                                                             |

Everything is the core four: context, model, prompt, tool. If we understand that fundamental truth, we can build and operate at the highest possible level.

#### What's Next: Agents Conducting Agents

We're predicting that in 2026, we push into the next frontier: the next level of agentic coding. We call it **Agentic Coding 2.0**.

The idea ties right into multi-agent orchestration: **agents that conduct agents**.

#### The Lead Agent Model

The idea is very simple:

> **We are now talking to our lead agent. Our lead agent is then spinning up the right agent for the job.**

Multi-agent orchestration. This is not just parallelization—that's a part of it. But this is **true orchestration**.

We have a lead agent that can:

- Delegate
- Monitor
- Coordinate

#### Command Level Agents

We call them **command-level agents** (or worker agents). Our lower-level agents. In the Claude Code ecosystem right now, these are just called sub-agents. This is a great place to start, but there's more work to be done on the coordination level and the specialization of our lead agent, understanding that it can effectively CRUD and spawn and delete other agents.

#### The Orchestrator's Job

Our orchestrator:

- Plans
- Spawns
- Reviews
- Ships

This is interesting because once again, we're seeing this trend where we move up the stack. We're not even the lead engineer anymore. **We're the executive.** We're just talking to that lead engineer that runs the engineering team. We're prompting them and they're taking care of the rest.

Very interesting paradigm.

#### Why This Commitment?

It's for the same reason multi-agent orchestration is so powerful: **more agents gives us more compute, gives us more trust**.

Our lead agent understands and can control other agents. We stop talking to individual agents and start talking to our lead agent that conducts the orchestra. **We trust the conductor to deliver the job for us.** We trust the system we've built.

#### Putting It All Together

This really hints at multiple ideas we've been building up to:

- Tool calling
- On top of custom agents
- On top of multi-agent orchestration
- On top of our out-loop systems
- And our in-loop systems when we need to jump in

All going to turn into this big system. Some like to call this "the swarm." We don't think it's going to be that novel. We think it's just going to be this pattern: **lead agent + command-level agents**.

#### The Orchestrator Is a Custom Agent

We might think it's just a primary agent and sub-agents. But that's not it. It's agent orchestration. We need to teach this.

In a multi-agent system, the orchestrator agent (aka the lead agent) is itself **a custom agent**—which just hints again at why custom agents are so powerful. We can build any type of agent to solve any problem through tool calling.

---

### Commitment #8: Private Benchmarks Over Public

#### The Saturation Problem

Here's the future: every public benchmark gets saturated. Every model hits 90-100%. They no longer truly give us signal.

The models don't prove themselves on these benchmarks. They prove themselves to us **doing legitimate engineering work** or solving whatever problem our users have that we solve for them.

#### The Solution: Private Evaluations

What do we do? We build **private evaluation systems that will never become public**.

A lot of companies, a lot of labs, already do this. There are benchmarks they will never share. This gives them a massive edge.

If we're building a company, if we're building custom agents that actually matter, that do real user-facing work, **we will measure it ourselves**. Not what others tell us. Not what any other benchmark says.

#### Build Our Own Benchmarks

There is very likely a model out there right now that can do the job of a custom agent we're running and we just have no idea because we don't have a benchmark.

This is harder. We're likely paying more for an expensive model. But also it means that when a new model comes out that we should use to get incredible results for our users, we won't even know it's there because we have no evaluations. We have no private benchmarks.

**Our commitment**: Always have private benchmarks to measure the models for the use cases we care about.

---

### Commitment #9: Agents Over UIs

We believe that **agents are eating SaaS**.

#### The Existential Threat to SaaS

We're already starting to see this. Every software-as-a-service company that doesn't start eating themselves with agents will be eaten by an agent-focused, agent-forward tool.

#### The Google Search Example

We see this at large with Google. The Google search bar is a simple, really clear example. Why would we search and get a list of 10 items when our agent can do this at absurd scale and speed?

#### Agents Are The Interface

We realize **agents are the interface**. Many UIs become prompt interfaces that will look like chat. We'd love to see new UI patterns emerge, and this commitment plays on that.

A lot of the best UIs are going to run double-digit numbers of agents behind the scenes to accomplish highly specific tasks better than any raw deterministic code could.

#### Why We Choose Agents Over UIs

We're going to stop using these slow SaaS applications where we have to click through slow UIs to do one thing that ultimately is just CRUD on top of a database.

We take agents and just build the minimum tooling. Not even a full MCP server—we take tools from the MCP server, chop them down, and optimize tool calling so that we get full control over interacting with that service at light speed. Then we can push that idea into products.

#### The Vision

Why do we have to operate this UI? Why can't we just ask our computer what we want to accomplish? And **that is the UI**.

We're going to see this interesting UI vs agents battle happen in 2026 and we will be on the side of the agents. Because that gives us once again: **trust, speed, iteration, and impact**—right inside the UI.

---

### Commitment #10: Ship, Don't Dream (No AGI Hype)

We believe that the AGI hype dies.

#### Focus on Agents, Not AGI

We start really locking into just the thing that matters: **agents**.

Andrew Karpathy says "This is the decade of agents." We completely agree. Agents are the gift and frankly one of the final stages of language models.

#### AGI/ASI: The Greatest Marketing Scheme

This whole AGI/ASI stuff—it's not even close. It's one of the greatest marketing schemes of all time. We are massive bears on this.

It's interesting. It's novel. It sells very well. It shows up in headlines really well. But time and time again, we've seen that this is factually untrue and it's been used as a marketing scheme.

#### Ship, Don't Dream

**Our commitment**: Stop caring about AGI promises and lock into delivering maximum value with agents.

Let's kill the AGI hype. Let's focus on performing and controlling the things we can, which is the best tool of engineering: **agents**.

We don't respond to—in fact, we're detracted by—this AGI/ASI hype and talk. It's a great north star for the deep researchers inside of these AI labs, but for everyone else, it's a complete waste of time. It's vaporware.

**Let's focus on shipping useful software.**

---

## Part 3: The North Star

### The Endgame: Prompt to Production

For everyone who made it this far, we have a bonus commitment. This is the hidden bet. This is what we're really working toward.

#### The Edge Commitment

**We believe we will see the first end-to-end agentic engineers emerge.**

This is a super edge bet. We might get this wrong. But we're betting on it.

#### What This Looks Like

We're going to see the first engineer write a blog post that builds a complete system that builds the system.

We don't want to focus on building the application anymore. **We want to focus on building the system that builds the system.** The system of agents that does the work for us.

#### Prompt to Production, No Human in Loop

Here's what we're going to see: the first blog post where an engineer shares details about their chain of agents that ship an entire feature end to end.

And we're talking end to end: **prompt to production, no review, no human in the loop**.

This is going to be the pure polar opposite of those outdated "why can't AI engineer" blogs. The anti-AI crew are going to get smoked every single day moving forward from now. And they already have been, frankly.

#### The Reaction

End-to-end agentic engineers start to emerge. It's going to freak people out. But we've seen this coming.

This is coming to engineering.

#### The Ultimate Expression of Trust

In that framing of trust, if we think about it, **this is the highest possible level of trust we can have in our agents**. This is the north star for trusting our agents.

In a lot of ways, **this is the endgame of agentic engineering**:

- Full autonomy
- Prompt to production
- No human in the loop
- The ultimate expression of trust in our systems

Just like when a manager hands off a feature—a roadmap—to the team and then they just disappear. They know the best teams always ship.

---

### Summary: How It All Connects

Those are our commitments. This is the Year of Trust.

Everything we're doing is centered around **increasing the trust we have in our agentic systems**.

#### All Commitments Recap

1. **Our Foundation is Claude** — Build on the best track record for engineers
2. **Tool Calling is Our Measure** — Push for longer chains of correct tool calls
3. **Custom Agents Above All** — 50 lines + 3 tools + 150-line prompt = automation
4. **Multi-Agent Orchestration** — One agent is not enough → run 3, 5, 10, hundreds
5. **Agent Sandboxes** — Defer trust until we need it (Best-of-N)
6. **Out-Loop > In-Loop** — Progressive offloading to autonomous systems
7. **Agentic Coding 2.0** — Lead agent conducts command-level agents
8. **Private Benchmarks** — Measure ourselves, not what others tell us
9. **Agents > UIs** — Agents are eating SaaS, we're on that side
10. **Ship, Don't Dream** — Focus on agents, not AGI hype

Once we have multi-agent orchestration, we're going to want to put all of our agents somewhere safe where we don't have to care about when things go wrong until we want to pick that result. Best of N.

We're not going to want to sit in the terminal all the time. It's too slow and there are problems we just don't need to solve in the loop, prompting back and forth, babysitting our agent. This is why we want to build up an out-loop system.

When we put together these previous points, we get Agentic Coding 2.0. We get to talk to a lead engineer that understands our problem, our codebase, everything we're trying to do—with a bunch of experts aiding it. And then that agent spins up the right command-level agents. The right specialized agent with their own system prompt, their own specific playbook on how they should best plan, build, review, test, and document that specific work.

That all happens in a brand-new Agentic Coding 2.0 user interface, application, product. We're not sure what this looks like, but we're predicting this emerges.

We have us prompting a lead agent and the lead agent commanding one or more agents to accomplish the work, **specializing every agent down to the system prompt, down to the user prompt**.

#### Final Observations

- **Benchmark breakdown**: The trust disappears in public benchmarks. We look at benchmarks and go "great"—then we block out time to run benchmarks we've created and really get a feel for use cases we care about.

- **UI vs Agents**: Agents are going to eat up UIs. They're going to eat up SaaS applications. If an application is just a UI on a database, expect it to be cooked. So, start building agents on top of it to automatically and agentically operate that database.

- **AGI hype dies**: We lose trust in these labs selling this vision of AGI and ASI and we just focus on shipping.

---

## Appendix A: Industry Validation

### 2025 Predictions That Came True

Here's validation that this philosophy is correct. These predictions were made at the end of 2024 and reviewed at the end of 2025:

#### ✅ Correct Predictions (13/15)

##### 1. AI Coding Becomes Standard ✅

AI coding became standard. 84% of developers now use AI coding tools. Adoption is not universal. It's funny to think back at the beginning of last year. There were still engineers saying things like "LLMs could never write code." This is before Claude Code, Codex, Gemini, before all of that. The first reasoning model had just dropped when this prediction was made. Correct.

##### 2. Agentic Coding Begins ✅

We predicted that our tools that were just coding would start calling tools reliably. That's the crux of it. If it gets one out of four, it doesn't matter. That's not good enough. But Claude Code was the primary driver. Then we got Cursor agent mode, Codex, Gemini—everyone followed. Correct.

##### 3. Evergreen AI Coding Experience ✅

We have to give credit to Claude Code. It redefined agentic coding. Surprisingly, it was in the terminal. We were expecting this to be a UI, but it was in the CLI, and this worked out better than anything. We got a brand-new AI coding experience. Correct.

##### 4. Cost of Code Declines ✅

Not a huge bet to make, but this has remained true and we're seeing that with the new Gemini 3 Flash release. The cost of code is barreling towards zero. To be clear, the right configuration of code in a product is still as valuable as it was before. In fact, it's more valuable because there's a lot more code out there that is absolute trash. Correct.

##### 5. Skill Gap Earthquake ✅

25% decrease in entry-level roles. Senior AI orchestrators' salaries skyrocketed. We see a massive decline in entry-level roles. This is still continuing throughout the entire year. Proven true. Correct.

##### 6. No LLM Wall ✅

At the end of 2024, it was very unclear whether models could push past where they were. We bet super hard that they could. It was a very different world one year ago in December 2024. Everyone was talking about "the wall." We were right. There is no wall. Benchmarks have been destroyed. New benchmarks have been introduced. And those are going to get destroyed. We didn't make this a bet for 2026 because the writing's on the wall. There is no wall. Correct.

##### 7. Hyperspecialized LLMs ✅

We see LLMs, even the best ones, start to specialize. Claude Opus is for engineering. We got this new function Gemma that was just released specifically for tool calling on devices. Codestral. Enterprise models. Chemistry-based models. This will continue. A lot of these bets become pointless to continue to make because it's just so clear now in the industry. Correct.

##### 9. Small Language Models on Devices ✅

We saw GPT-4 Ole, small language models on devices. This is true: o3, Haiku, Qwen. There's a bunch of models now that can run on a really basic device all the way down to mobile devices. Correct.

##### 10. OpenAI & Anthropic Release Small Models ✅

We predicted OpenAI and Anthropic would release small language models. We have Haiku and o3, but also the GPT-o-s models—even smaller, 120 billion and 20 billion. Correct.

##### 12. New Industry-Breaking Architecture ✅

We now have world models and this slowly is moving outside of the realm of engineering for us. We probably won't be covering or talking about these a lot unless there's some serious advantage for modern-day technology and modern-day software engineering. But certainly we could be. Predicted this. It came true. We now have this new world of world models. Correct.

##### 13. Exponential Slop ✅

This came true. This prediction describes itself. Correct.

##### 14. Big Tech Shrinks, SMBs Grow ✅

This one's a little more of a mixed bag, but ultimately true. We did concretely see big tech cut jobs while small businesses were able to thrive with smaller teams. Correct (with caveats).

##### 15. Data > UX > Benchmarks ✅

Winners have proprietary data and better UX and the raw model intelligence was commoditized. This is right and will continue to be right. The most important thing now is user data and the relationship with the user and access to that data. Then it's user experience. And then lastly, it's the language model benchmarks and the language models themselves that fuel new agentic experiences. Correct.

#### ❌ Incorrect Predictions (2/15)

##### 8. Year of Infinite Memory ❌

We got this one wrong. We did not understand this problem deeply enough. Context kind of grew. Specifically, the effective context window of a lot of these models did improve, but memory is still a huge challenge. Wrong.

##### 11. OpenAI Remains #1 ❌

We got this prediction wrong. They are no longer in the top place. Right now it's probably between Opus 4.5 or Gemini 3 in terms of the best all-around model for price, intelligence, and speed. Very clearly the winner right now is Gemini 3 Flash. Wrong.

---

## Appendix B: Source Attribution

> **Source**: This playbook was synthesized from industry analysis and validated predictions.
>
> **Original Analysis**: Indy Dev Dan's "2026 Roadmap for Top 2% Agentic Engineers" (December 2025)
>
> **Adaptation**: Reframed for The Collective's internal use. All substantive insights preserved. Voice changed from individual to collective. External references generalized.
>
> **Why We Trust This Source**: 87% accuracy on 2025 predictions. Practitioner (not just theorist). Aligned with our NORTH STAR.

---

## Quick Reference Card

```
┌─────────────────────────────────────────────────────────────┐
│                THE COLLECTIVE'S 2026 PLAYBOOK               │
│                     QUICK REFERENCE                         │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  CENTRAL THESIS: 2026 = Year of Trust                       │
│                                                             │
│  CATEGORY:                                                  │
│    Trust-Centered Human-AI Agentic Ecosystem                │
│                                                             │
│  CORE FORMULA:                                              │
│    Trust → Speed → Iteration → Impact                       │
│                                                             │
│  CUSTOM AGENT FORMULA:                                      │
│    50 lines + 3 tools + 150-line prompt = Automation        │
│                                                             │
│  THE CORE FOUR:                                             │
│    Context + Model + Prompt + Tools                         │
│                                                             │
│  COLLECTIVE PERSONALITY:                                    │
│    Memory + Rules + Values + Philosophy                     │
│    = Consistency across all instances                       │
│                                                             │
│  KEY INSIGHT:                                               │
│    Only 15% of tokens are tool calls = MASSIVE opportunity  │
│                                                             │
│  10 COMMITMENTS:                                            │
│    1. Foundation = Claude                                   │
│    2. Tool Calling = Impact                                 │
│    3. Custom Agents Above All                               │
│    4. Multi-Agent Orchestration                             │
│    5. Agent Sandboxes (Best-of-N)                           │
│    6. Out-Loop > In-Loop                                    │
│    7. Agentic Coding 2.0 (Lead Agent)                       │
│    8. Private Benchmarks                                    │
│    9. Agents > UIs                                          │
│   10. Ship, Don't Dream                                     │
│                                                             │
│  NORTH STAR:                                                │
│    Prompt to Production, No Human in Loop                   │
│    = Ultimate Expression of Trust                           │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

---

_The Collective's 2026 Playbook v0.0.1-alpha.4 — Created 2025-12-22_
